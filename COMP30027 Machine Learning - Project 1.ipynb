{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2018 Semester 1\n",
    "-----\n",
    "## Project 1: What is labelled data worth to Naive Bayes?\n",
    "-----\n",
    "###### Student Name: Emmanuel Macario\n",
    "###### Student Number: 831659\n",
    "###### Email: <macarioe@student.unimelb.edu.au>\n",
    "###### Python version: 3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(filename):\n",
    "    \"\"\"\n",
    "    Opens a data file in csv format, and transforms it \n",
    "    into a usable format, in the form of a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read csv data into a dataframe. Assign\n",
    "    # an integer value to each attribute in\n",
    "    # the data.\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "        \n",
    "    \n",
    "    # Partition the data into a list of instances, and \n",
    "    # a list of class labels for those instances\n",
    "    instance_list = df.iloc[:,:-1]\n",
    "    class_list = df.iloc[:,-1]\n",
    "    data_set = instance_list, class_list\n",
    "    \n",
    "    \n",
    "    return data_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def proportion_missing_values(instance_list):\n",
    "    \"\"\"\n",
    "    Calculates and returns the proportion of \n",
    "    instances that contain at least one missing value\n",
    "    in the entire dataset. Used for personal analysis \n",
    "    of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(instance_list[instance_list.values == '?'])/len(instance_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_supervised(instance_list, class_list):\n",
    "    \"\"\"\n",
    "    Builds a supervised Naive Bays model, given a \n",
    "    preprocessed set of training data, by calculating \n",
    "    counts from the training data.\n",
    "    \n",
    "    Takes as input a training set of data, consisting\n",
    "    of a list of instances, and a list of class labels \n",
    "    corresponding to those instances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The supervised Naive Bays model\n",
    "    supervised_model = {}\n",
    "    \n",
    "    # For every instance in the training\n",
    "    # data, update the model\n",
    "    for i in range(len(instance_list)):\n",
    "        \n",
    "        # Get the instance\n",
    "        instance = instance_list.iloc[i,:]\n",
    "        \n",
    "        # Get the associated class label\n",
    "        class_label = class_list[i]\n",
    "        \n",
    "        \n",
    "        # If the class label is not already in\n",
    "        # the model, create a nested set of\n",
    "        # dictionaries for the class.\n",
    "        if class_label not in supervised_model:\n",
    "            \n",
    "            supervised_model[class_label] = {}\n",
    "            \n",
    "            # For each attribute, initialise\n",
    "            # a new default dictionary.\n",
    "            for attr in instance_list.columns:\n",
    "                supervised_model[class_label][attr] = defaultdict(int)\n",
    "                \n",
    "        \n",
    "        # Then, for every attribute in the instance,\n",
    "        # get its corresponding value and update\n",
    "        # the model.\n",
    "        for attr in instance_list.columns:\n",
    "            \n",
    "            attr_value = instance[attr]\n",
    "            \n",
    "            # For missing values in a training instance, it is\n",
    "            # possible to simply have them not contribute to the\n",
    "            # attribute-value counts.\n",
    "            if attr_value == '?':\n",
    "                continue\n",
    "                \n",
    "            supervised_model[class_label][attr][attr_value] += 1\n",
    "    \n",
    "    \n",
    "    return supervised_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_supervised(supervised_model, instance_list):\n",
    "    \"\"\"\n",
    "    Predicts the class labels for a set of test\n",
    "    instances, based on a supervised Naive Bayes\n",
    "    model. Implements epsilon probabilistic smoothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Epsilon value for epsilon-smoothing\n",
    "    EPSILON = np.finfo(float).eps\n",
    "    \n",
    "    \n",
    "    # The list of predictions for each instance\n",
    "    prediction_list = []\n",
    "    \n",
    "    \n",
    "    # Get the class frequencies to avoid redundant calculations\n",
    "    class_freqs = get_class_freqs(supervised_model)\n",
    "    \n",
    "    \n",
    "    # Calculate the total number of instances,\n",
    "    total_instances = sum(class_freqs.values())\n",
    "    \n",
    "    \n",
    "    # For each instance in the test set, predict \n",
    "    # its class based on the supervised model.\n",
    "    for instance in instance_list.values:\n",
    "        \n",
    "        max_prob = 0.0\n",
    "        predicted_class = \"\"\n",
    "        \n",
    "        # We obtain the probability of an instance being a \n",
    "        # certain class, P(C|X), for each possible class.\n",
    "        for class_label in class_freqs:\n",
    "            \n",
    "            # Firstly, get the prior probability of a class, P(Cj)\n",
    "            prob = class_freqs[class_label]/total_instances\n",
    "            \n",
    "            \n",
    "            # Now, multiply the prior probability of the class\n",
    "            # by the posterior probability of each of the instance's \n",
    "            # attribute values, given the class, P(Xi|Cj).\n",
    "            for attr in supervised_model[class_label]:\n",
    "                \n",
    "                # Get the attribute value\n",
    "                attr_value = instance[attr]\n",
    "                \n",
    "                # If an attribute value is missing for the training\n",
    "                # instance, simply have it not count towards the probability\n",
    "                # estimates\n",
    "                if attr_value == '?':\n",
    "                    continue\n",
    "                \n",
    "                # When accessing values, if value is 0, replace with epsilon\n",
    "                attr_prob = supervised_model[class_label][attr].get(attr_value, EPSILON) / class_freqs[class_label]\n",
    "                \n",
    "                # Update the probability estimate\n",
    "                prob *= attr_prob\n",
    "                \n",
    "            \n",
    "            # If the probability is the highest seen\n",
    "            # thus far, set the predicted class to the\n",
    "            # class label.\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                predicted_class = class_label\n",
    "            \n",
    "        \n",
    "        # Add the class label with the highest\n",
    "        # corresponding probability to the list\n",
    "        # of predictions.\n",
    "        prediction_list.append(predicted_class)\n",
    "        \n",
    "        \n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_freqs(supervised_model):\n",
    "    \"\"\"\n",
    "    Helper function that returns a dictionary containing \n",
    "    frequency counts of all the class labels in the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_freqs = defaultdict(int)\n",
    "    \n",
    "    for class_label in supervised_model:\n",
    "        class_freqs[class_label] = sum(supervised_model[class_label][0].values())\n",
    "        \n",
    "    return class_freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_supervised(prediction_list, class_list):\n",
    "    \"\"\"\n",
    "    Evaluates a set of predictions, in a supervised\n",
    "    context. Uses accuracy as the primary method of\n",
    "    evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation checking\n",
    "    assert(len(prediction_list) == len(class_list))\n",
    "    \n",
    "    # Calculate and return the accuracy of the model\n",
    "    return (prediction_list == class_list).value_counts().loc[True] / len(prediction_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_unsupervised(instance_list, class_labels, class_distributions=None):\n",
    "    \"\"\"\n",
    "    Builds a weak unsupervised Naive Bayes model,\n",
    "    from a given set of unlabelled training data,\n",
    "    and a unique set of possible class labels. This\n",
    "    function is called by predict_unsupervised() to\n",
    "    iteratively update the random class distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the random class distributions have not been\n",
    "    # created, then for each training instance, create\n",
    "    # a non-uniform (random) class distribution.\n",
    "    if class_distributions is None:\n",
    "        \n",
    "        random_class_distributions = []\n",
    "\n",
    "        for instance in instance_list.values:\n",
    "\n",
    "            random_distribution = np.abs(np.random.randn(1, len(class_labels))[0])\n",
    "            norm_random_distribution = random_distribution / sum(random_distribution)\n",
    "            random_class_distributions.append(norm_random_distribution)\n",
    "\n",
    "        # Store all random class distributions in a dataframe\n",
    "        class_distributions = pd.DataFrame(random_class_distributions, \n",
    "                                           columns=class_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Initialise the unsupervised Naive Bays model, \n",
    "    # which is just a set of nested dictionaries\n",
    "    unsupervised_model = {c : {a : defaultdict(float) for a in instance_list.columns} for c in class_labels}\n",
    "    \n",
    "    \n",
    "    # For every instance in the training set, update\n",
    "    # fractional counts in the unsupervised model\n",
    "    for i in range(len(instance_list)):\n",
    "        \n",
    "        for class_label in class_labels:\n",
    "            \n",
    "            for attr in instance_list.columns:\n",
    "            \n",
    "                attr_value = instance_list[attr][i]\n",
    "                \n",
    "                # If an attribute value is missing,\n",
    "                # simply have it not contribute to counts\n",
    "                if attr_value == '?':\n",
    "                    continue\n",
    "                \n",
    "                unsupervised_model[class_label][attr][attr_value] += class_distributions[class_label][i]\n",
    "                \n",
    "    \n",
    "    return class_distributions, unsupervised_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_unsupervised(class_distributions, unsupervised_model, instance_list, iterations=5):\n",
    "    \"\"\"\n",
    "    Predicts the class distribution for a set of\n",
    "    instances, based on a trained model. Delegates\n",
    "    the task of updating class distributions in a\n",
    "    single iteration to the train_unsupervised() function.\n",
    "    Once the 'final' class distributions have been calculated,\n",
    "    it classifies the instances based on the distributions.\n",
    "    \"\"\"\n",
    "        \n",
    "    # For each iteration, update the class distribution for each\n",
    "    # instance, in an attempt to make them more reliable than\n",
    "    # the initial random distribution.\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Calculate the frequency of the classes \n",
    "        # in the random class distribution\n",
    "        class_freqs = get_class_freqs2(class_distributions)\n",
    "         \n",
    "        \n",
    "        # Calculate the total number of instances\n",
    "        total_instances = int(sum(class_freqs.values()) + 0.5)\n",
    "    \n",
    "        \n",
    "        for i in range(len(instance_list)):\n",
    "\n",
    "            instance = instance_list.iloc[i,:]\n",
    "    \n",
    "            for class_label in class_freqs:\n",
    "\n",
    "                # Firstly, get the prior probability of a class\n",
    "                prob = class_freqs[class_label] / total_instances\n",
    "\n",
    "                # Now, multiply the prior probability of the class\n",
    "                # by the posterior probability of each of the instance's \n",
    "                # attribute values, given the class.\n",
    "                for attr in unsupervised_model[class_label]:\n",
    "                    \n",
    "                    attr_value = instance[attr]\n",
    "                    \n",
    "                    # Simply not have missing values count\n",
    "                    # towards probability estimates.\n",
    "                    if attr_value == '?':\n",
    "                        continue\n",
    "                    \n",
    "                    prob *= (unsupervised_model[class_label][attr][attr_value] / class_freqs[class_label])\n",
    "\n",
    "\n",
    "                # Update the class distribution for the instance\n",
    "                class_distributions[class_label][i] = prob\n",
    "\n",
    "\n",
    "        # Now, normalise the class distributions so that the \n",
    "        # probabilities add to 1 for every single distribution.\n",
    "        class_distributions = class_distributions.div(class_distributions.sum(axis=1), axis=0)\n",
    "        \n",
    "\n",
    "        # Finally, update the unsupervised model based on the updated class distributions\n",
    "        class_distributions, unsupervised_model = train_unsupervised(instance_list, \n",
    "                                                                     class_distributions.columns, \n",
    "                                                                     class_distributions)\n",
    "        \n",
    "    \n",
    "    # The list of class predictions for the instances\n",
    "    prediction_list = []\n",
    "    \n",
    "    \n",
    "    # Assign the class with the highest probability in\n",
    "    # the final distribution as the predicted class, and\n",
    "    # add it to the list of predictions.\n",
    "    for distribution in class_distributions.values:\n",
    "        \n",
    "        predicted_class = class_distributions.columns[np.argmax(distribution)]\n",
    "        \n",
    "        prediction_list.append(predicted_class)\n",
    "            \n",
    "            \n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_freqs2(class_distributions):\n",
    "    \"\"\"\n",
    "    Helper function that returns a dictionary\n",
    "    containing classes as keys, and the total\n",
    "    sum of the fractional values for each class\n",
    "    as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_freqs = defaultdict(float)\n",
    "    \n",
    "    for class_label in class_distributions.columns:\n",
    "        class_freqs[class_label] = class_distributions[class_label].sum()\n",
    "        \n",
    "    return class_freqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_unsupervised(prediction_list, class_list):\n",
    "    \"\"\"\n",
    "    Evaluates a set of predictions, in an\n",
    "    unsupervised manner. Takes as input a set of \n",
    "    final class distributions for each instance, and \n",
    "    the actual class for each respective instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation checking\n",
    "    assert(len(prediction_list) == len(class_list))\n",
    "    \n",
    "    \n",
    "    # Initialise and fill in the confusion matrix\n",
    "    confusion_matrix = {predicted_class : defaultdict(int) for predicted_class in set(prediction_list)}\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        predicted_class = prediction_list[i]\n",
    "        actual_class = class_list[i]\n",
    "        confusion_matrix[predicted_class][actual_class] += 1\n",
    "    \n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix\\n================\")\n",
    "    for predicted_class in confusion_matrix:\n",
    "        print(\"{:20s}:\".format(predicted_class) + \"   {0}\".format(confusion_matrix[predicted_class]))\n",
    "    \n",
    "    \n",
    "    # Calculate the total number of 'correct' predictions. We\n",
    "    # take the max value from each column in the confusion matrix,\n",
    "    # and add it to the total number of correct predictions.\n",
    "    total_correct = 0\n",
    "    for predicted_class in confusion_matrix:\n",
    "        total_correct += max(confusion_matrix[predicted_class].values())\n",
    "    \n",
    "    \n",
    "    # Finally, calculate accuracy of the classifier\n",
    "    accuracy = total_correct / len(prediction_list)\n",
    "    \n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Functions\n",
    "These functions are drivers for both supervised and unsupervised Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "================\n",
      "e                   :   defaultdict(<class 'int'>, {'e': 1896, 'p': 1670})\n",
      "p                   :   defaultdict(<class 'int'>, {'p': 2246, 'e': 2312})\n",
      "Accuracy: 0.5179714426\n"
     ]
    }
   ],
   "source": [
    "# Driver for supervised Naive Bayes classification\n",
    "\n",
    "def supervised_naive_bayes(filename):\n",
    "    \"\"\"\n",
    "    Creates a supervised Naive Bayes model\n",
    "    given a set of input data, classifies\n",
    "    the instances based on the probabilistic \n",
    "    model, then prints the accuracy of the \n",
    "    classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    instance_list, class_list = preprocess(filename)\n",
    "    supervised_model = train_supervised(instance_list, class_list)\n",
    "    prediction_list = predict_supervised(supervised_model, instance_list)\n",
    "    accuracy = evaluate_supervised(prediction_list, class_list)\n",
    "    print(\"Accuracy: {:.10f}\".format(accuracy))\n",
    "    \n",
    "    \n",
    "# Driver for unsupervised Naive Bayes classification\n",
    "    \n",
    "def unsupervised_naive_bayes(filename):\n",
    "    \"\"\"\n",
    "    Creates a weak unsupervised Naive Bayes model\n",
    "    given a set of training instances, classifies\n",
    "    the instances by iteratively updating an initial\n",
    "    set of random class distributions, then\n",
    "    prints the accuracy of the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    instance_list, class_list = preprocess(filename)\n",
    "    class_labels = class_list.unique()\n",
    "    class_distributions, unsupervised_model = train_unsupervised(instance_list, class_labels)\n",
    "    prediction_list = predict_unsupervised(class_distributions, unsupervised_model, instance_list)\n",
    "    accuracy = evaluate_unsupervised(prediction_list, class_list)\n",
    "    print(\"Accuracy: {:.10f}\".format(accuracy))\n",
    "    \n",
    "    \n",
    "# Filename constants used for easy file access\n",
    "CSV1 = 'breast-cancer-dos.csv'\n",
    "CSV2 = 'car-dos.csv'\n",
    "CSV3 = 'hypothyroid-dos.csv'\n",
    "CSV4 = 'mushroom-dos.csv'\n",
    "\n",
    "\n",
    "# Testing driver functions\n",
    "filename = CSV4\n",
    "\n",
    "unsupervised_naive_bayes(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "\n",
    "### Question 1\n",
    "Since we’re starting off with random guesses, it might be surprising that the unsupervised NB works at all. Explain what characteristics of the data cause it to work pretty well (say, within 10% Accuracy of the supervised NB) most of the time; also, explain why it utterly fails sometimes.\n",
    "\n",
    "\n",
    "**Answer:** _The unsupervised classifier works well on the datasets that have both a small number of distinct classes, and a small number of distinct values for their attributes. Furthermore, the classifier achieves lower variance in accuracy for datasets containing instances that are partitioned non-uniformly among classes._\n",
    "\n",
    "_For example, the classifier shows high variance in accuracy for the mushroom data with a mean of 0.813, a low of 0.518 and a high of 0.895. This variance could be the result of many attributes having a high number of unique values, with a max of 12 unique values for attribute 8, and a mean of 5.32 unique values per attribute. Compounded with the fact that the training data is partitioned pretty evenly among two classes (4208 instances of 'e', and 3916 of 'p'), this could have made it difficult for the classifier to converge to a particular set of predictions, hence the randomness of the results._\n",
    "\n",
    "_Juxtaposed, consistent results are shown for the hypothyroid and breast cancer datasets, with mean accuracies of 0.952 and 0.703 respectively. 17 of 18 attributes in the hypothyroid data have only 2 possible unique values. A possible further reason for low variance could be that the distribution of instances among classes is highly non-uniform for both datasets. This could possibly inflate accuracy, since accuracy is derived by taking the maximum value from each column in the confusion matrix. If most instances are of one class, then these values tend to be large._\n",
    "\n",
    "\n",
    "**Note to assessor: corresponding accuracies for supervised NB are detailed in the response to question two.**\n",
    "\n",
    "### Question 2\n",
    "When evaluating supervised NB across the four different datasets, you will observe some variation in effectiveness (e.g. Accuracy). Explain what causes this variation. Describe and explain any particularly suprising results.\n",
    "\n",
    "\n",
    "**Answer:** _The accuracies of the supervised classifier across the datasets can be seen below:_\n",
    "\n",
    "| Dataset        | Accuracy      |\n",
    "| -------------  |:-------------:|\n",
    "| _breast-cancer_| 0.755         | \n",
    "| _car_          | 0.874         |\n",
    "| _hypothyroid_  | 0.952         |\n",
    "| _mushroom_     | 0.997         |\n",
    "\n",
    "\n",
    "_Since accuracy measures the ratio of correctly classified instances across all classes, this means that if one class occurs more than others, then the resulting accuracy is clearly dominated by the accuracy of the dominating class. This is the case for the hypothyroid dataset, since the classifier predicts 'negative' for all instances, when the training set is primarily dominated by 'negative' instances._\n",
    "\n",
    "_A surprising result was the accuracy of 0.997 for the mushroom data. Such a high accuracy was not expected since 30.5% of instances contained at least one missing value. The reason a high accuracy was received is due to there being a large number of training instances containing a broad distribution of attribute value-class pairs, and because Naive Bayes gracefully degrades when one or more predictor variables are missing or not observed. The latter is due to the implementation of epsilon smoothing and simply not having missing values contribute to frequency counts for their respective features._\n",
    "\n",
    "_Compared to the other results, the accuracy of 0.755 for the breast cancer data was relatively low. Unlike the other datasets, the training set was relatively small, perhaps leading to a probabilistic model that did not cover a wide spectrum of attribute value-class pairs. This could have possibly led to the moderately low prediction accuracy._\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
