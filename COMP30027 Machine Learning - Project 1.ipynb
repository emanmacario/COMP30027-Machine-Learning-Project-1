{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2018 Semester 1\n",
    "-----\n",
    "## Project 1: What is labelled data worth to Naive Bayes?\n",
    "-----\n",
    "###### Student Name: Emmanuel Macario\n",
    "###### Student Number: 831659\n",
    "###### Email: <macarioe@student.unimelb.edu.au>\n",
    "###### Python version: 3.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the seven functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import useful libraries\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filename constants used for easy file access\n",
    "CSV1 = 'breast-cancer-dos.csv'\n",
    "CSV2 = 'car-dos.csv'\n",
    "CSV3 = 'hypothyroid-dos.csv'\n",
    "CSV4 = 'mushroom-dos.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-recurrence-events    201\n",
      "recurrence-events        85\n",
      "Name: 9, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(filename):\n",
    "    \"\"\"\n",
    "    Opens a data file in csv, and transforms it into\n",
    "    a usable format, in the form of a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read csv data into a dataframe. Assign\n",
    "    # an integer value to each attribute in\n",
    "    # the data.\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "        \n",
    "    \n",
    "    # Partition the data into a list of instances, and \n",
    "    # a list of class labels for those instances\n",
    "    instance_list = df.iloc[:,:-1]\n",
    "    class_list = df.iloc[:,-1]\n",
    "    data_set = instance_list, class_list\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Give a description of the class distributions\n",
    "    print(\"CLASS DISTRIBUTION\")\n",
    "    print(pd.Series(class_list).value_counts())\n",
    "    \n",
    "    print()\n",
    "    print(\"ATTRIBUTE DISTRIBUTIONS\")\n",
    "    print(instance_list.describe())\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return data_set\n",
    "\n",
    "\n",
    "# Test preprocess function\n",
    "instance_list, class_list = preprocess(CSV1)\n",
    "\n",
    "print(pd.Series(class_list).value_counts())\n",
    "\n",
    "\n",
    "def proportion_missing_values(instance_list):\n",
    "    \"\"\"\n",
    "    Calculates and returns the proportion of \n",
    "    instances that contain at least one missing value\n",
    "    in the entire dataset. Used for personal analysis \n",
    "    of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return len(instance_list[instance_list.values == '?'])/len(instance_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_supervised(instance_list, class_list):\n",
    "    \"\"\"\n",
    "    Builds a supervised Naive Bays model,\n",
    "    given a preprocessed set of training\n",
    "    data, by calculating counts from the\n",
    "    training data.\n",
    "    \n",
    "    Inputs: a training set of data, consisting\n",
    "    of a list of instances, and a list of class \n",
    "    labels for those instances.\n",
    "    \n",
    "    Outputs: 2-tuple containing a class frequency \n",
    "    dictionary and also the supervised model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The supervised Naive Bays model\n",
    "    supervised_model = {}\n",
    "    \n",
    "    # For every instance in the training\n",
    "    # data, update the model\n",
    "    for i in range(len(instance_list)):\n",
    "        \n",
    "        # Get the instance\n",
    "        instance = instance_list.iloc[i,:]\n",
    "        \n",
    "        # Get the associated class label\n",
    "        class_label = class_list[i]\n",
    "        \n",
    "        \n",
    "        # If the class label is not already in\n",
    "        # the model, create a nested set of\n",
    "        # dictionaries for the class.\n",
    "        if class_label not in supervised_model:\n",
    "            \n",
    "            # New dictionary for every new class\n",
    "            supervised_model[class_label] = {}\n",
    "            \n",
    "            # For each attribute, initialise\n",
    "            # a new default dictionary.\n",
    "            for attr in instance_list.columns:\n",
    "                supervised_model[class_label][attr] = defaultdict(int)\n",
    "                \n",
    "        \n",
    "        # For every attribute in the instance,\n",
    "        # get its corresponding value and update\n",
    "        # the model.\n",
    "        for attr in instance_list.columns:\n",
    "            \n",
    "            attr_value = instance[attr]\n",
    "            \n",
    "            # For missing values in a training instance, it is\n",
    "            # possible to simply have them not contribute to the\n",
    "            # attribute-value counts.\n",
    "            if attr_value == '?':\n",
    "                continue\n",
    "                \n",
    "            supervised_model[class_label][attr][attr_value] += 1\n",
    "    \n",
    "    \n",
    "    return supervised_model\n",
    "\n",
    "\n",
    "\n",
    "# Test function\n",
    "supervised_model = train_supervised(instance_list, class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-recurrence-events    209\n",
      "recurrence-events        77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_supervised(supervised_model, instance_list):\n",
    "    \"\"\"\n",
    "    Predicts the class labels for a set of test\n",
    "    instances, based on a supervised Naive Bayes\n",
    "    model. Implements epsilon probabilistic smoothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Epsilon value for epsilon-smoothing\n",
    "    EPSILON = np.finfo(float).eps\n",
    "    \n",
    "    \n",
    "    # The list of predictions for each instance\n",
    "    prediction_list = []\n",
    "    \n",
    "    \n",
    "    # Get the class frequencies to avoid redundant calculations\n",
    "    class_freqs = get_class_freqs(supervised_model)\n",
    "    \n",
    "    \n",
    "    # Calculate the total number of instances,\n",
    "    total_instances = sum(class_freqs.values())\n",
    "    \n",
    "    \n",
    "    # For each instance in the test set, predict \n",
    "    # its class based on the supervised model.\n",
    "    for instance in instance_list.values:\n",
    "        \n",
    "        max_prob = 0.0\n",
    "        predicted_class = \"\"\n",
    "        \n",
    "        # We obtain the probability of an instance\n",
    "        # being a certain class, for each possible class.\n",
    "        for class_label in class_freqs:\n",
    "            \n",
    "            # Firstly, get the prior probability of a class\n",
    "            prob = class_freqs[class_label]/total_instances\n",
    "            \n",
    "            \n",
    "            # Now, multiply the prior probability of the class\n",
    "            # by the posterior probability of each of the instance's \n",
    "            # attribute values, given the class.\n",
    "            for attr in supervised_model[class_label]:\n",
    "                \n",
    "                # Get the attribute value\n",
    "                attr_value = instance[attr]\n",
    "                \n",
    "                # If an attribute value is missing for the training\n",
    "                # instance, simply have it not count towards the probability\n",
    "                # estimates\n",
    "                if attr_value == '?':\n",
    "                    continue\n",
    "                \n",
    "                # When accessing values, if value is 0, replace with epsilon\n",
    "                post_attr = supervised_model[class_label][attr].get(attr_value, EPSILON)\n",
    "                \n",
    "                # Update the probability estimate\n",
    "                prob *= (post_attr/class_freqs[class_label])\n",
    "                \n",
    "            \n",
    "            # If the probability is the highest seen\n",
    "            # thus far, set the predicted class to the\n",
    "            # class label.\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                predicted_class = class_label\n",
    "            \n",
    "        \n",
    "        # Add the class label with the highest\n",
    "        # corresponding probability to the list\n",
    "        # of predictions.\n",
    "        prediction_list.append(predicted_class)\n",
    "        \n",
    "        \n",
    "    return prediction_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_freqs(supervised_model):\n",
    "    \"\"\"\n",
    "    Helper function that returns a dictionary containing \n",
    "    frequency counts of all the class labels in the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_freqs = defaultdict(int)\n",
    "    \n",
    "    for class_label in supervised_model:\n",
    "        class_freqs[class_label] = sum(supervised_model[class_label][0].values())\n",
    "        \n",
    "    return class_freqs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the function\n",
    "prediction_list = predict_supervised(supervised_model, instance_list)\n",
    "\n",
    "print(pd.Series(prediction_list).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Supervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7552447552447552"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate_supervised(prediction_list, class_list):\n",
    "    \"\"\"\n",
    "    Evaluates a set of predictions, in a supervised\n",
    "    context. Uses accuracy as the primary method of\n",
    "    evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation checking\n",
    "    assert(len(prediction_list) == len(class_list))\n",
    "    \n",
    "    # Calculate and return the accuracy of the model\n",
    "    return (prediction_list == class_list).value_counts().loc[True] / len(prediction_list)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "evaluate_supervised(prediction_list, class_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[443.85470744229787, 429.7178964935455, 428.46440011018655, 425.962995953971]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_unsupervised(instance_list, class_labels, class_distributions=None):\n",
    "    \"\"\"\n",
    "    Builds a weak unsupervised Naive Bayes model,\n",
    "    from a given set of unlabelled training data,\n",
    "    and a unique set of possible class labels. This\n",
    "    function is called by predict_unsupervised() to\n",
    "    perform a single iteration to update the random \n",
    "    class distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the random class distributions have not been\n",
    "    # created, then for each training instance, create\n",
    "    # a non-uniform (random) class distribution for \n",
    "    # that instance.\n",
    "    if class_distributions is None:\n",
    "        \n",
    "        random_class_distributions = []\n",
    "\n",
    "        for instance in instance_list.values:\n",
    "\n",
    "            random_distribution = np.abs(np.random.randn(1, len(class_labels))[0])\n",
    "            norm_random_distribution = random_distribution / sum(random_distribution)\n",
    "            random_class_distributions.append(norm_random_distribution)\n",
    "\n",
    "        # Store all random class distributions in a dataframe\n",
    "        class_distributions = pd.DataFrame(random_class_distributions, \n",
    "                                           columns=class_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Initialise the unsupervised Naive Bays model, \n",
    "    # which is just a set of nested dictionaries\n",
    "    unsupervised_model = {c : {a : defaultdict(float) for a in instance_list.columns} for c in class_labels}\n",
    "    \n",
    "    \n",
    "    # For every instance in the training set, update\n",
    "    # fractional counts in the unsupervised model\n",
    "    for i in range(len(instance_list)):\n",
    "        \n",
    "        for class_label in class_labels:\n",
    "            \n",
    "            for attr in instance_list.columns:\n",
    "            \n",
    "                attr_value = instance_list[attr][i]\n",
    "                \n",
    "                # If an attribute value is missing,\n",
    "                # simply have it not contribute to counts\n",
    "                if attr_value == '?':\n",
    "                    continue\n",
    "                \n",
    "                unsupervised_model[class_label][attr][attr_value] += class_distributions[class_label][i]\n",
    "                \n",
    "    \n",
    "    return class_distributions, unsupervised_model\n",
    "\n",
    "\n",
    "\n",
    "# Test the function\n",
    "class_labels = class_list.unique()\n",
    "cd, um = train_unsupervised(instance_list, class_labels)\n",
    "print([cd[class_label].sum() for class_label in cd.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unacc</th>\n",
       "      <th>acc</th>\n",
       "      <th>vgood</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.258248</td>\n",
       "      <td>0.260228</td>\n",
       "      <td>0.222243</td>\n",
       "      <td>0.259281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246134</td>\n",
       "      <td>0.261741</td>\n",
       "      <td>0.223838</td>\n",
       "      <td>0.268288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.251462</td>\n",
       "      <td>0.266696</td>\n",
       "      <td>0.223327</td>\n",
       "      <td>0.258515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259809</td>\n",
       "      <td>0.259314</td>\n",
       "      <td>0.223804</td>\n",
       "      <td>0.257073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.247658</td>\n",
       "      <td>0.260859</td>\n",
       "      <td>0.225442</td>\n",
       "      <td>0.266041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.252995</td>\n",
       "      <td>0.265772</td>\n",
       "      <td>0.224906</td>\n",
       "      <td>0.256327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.258733</td>\n",
       "      <td>0.252528</td>\n",
       "      <td>0.220664</td>\n",
       "      <td>0.268075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246540</td>\n",
       "      <td>0.253938</td>\n",
       "      <td>0.222197</td>\n",
       "      <td>0.277325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.251994</td>\n",
       "      <td>0.258866</td>\n",
       "      <td>0.221792</td>\n",
       "      <td>0.267347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.268838</td>\n",
       "      <td>0.257342</td>\n",
       "      <td>0.225160</td>\n",
       "      <td>0.248659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.256448</td>\n",
       "      <td>0.259061</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>0.257519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.261854</td>\n",
       "      <td>0.263818</td>\n",
       "      <td>0.226327</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.270414</td>\n",
       "      <td>0.256390</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.246496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.258140</td>\n",
       "      <td>0.228556</td>\n",
       "      <td>0.255315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.263402</td>\n",
       "      <td>0.262856</td>\n",
       "      <td>0.227886</td>\n",
       "      <td>0.245856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.269418</td>\n",
       "      <td>0.249796</td>\n",
       "      <td>0.223622</td>\n",
       "      <td>0.257164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.256946</td>\n",
       "      <td>0.251411</td>\n",
       "      <td>0.225373</td>\n",
       "      <td>0.266270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.262480</td>\n",
       "      <td>0.256143</td>\n",
       "      <td>0.224834</td>\n",
       "      <td>0.256543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.266064</td>\n",
       "      <td>0.241747</td>\n",
       "      <td>0.236222</td>\n",
       "      <td>0.255967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.253707</td>\n",
       "      <td>0.243272</td>\n",
       "      <td>0.238033</td>\n",
       "      <td>0.264988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.259225</td>\n",
       "      <td>0.247901</td>\n",
       "      <td>0.237513</td>\n",
       "      <td>0.255361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.267609</td>\n",
       "      <td>0.240840</td>\n",
       "      <td>0.237824</td>\n",
       "      <td>0.253727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.255217</td>\n",
       "      <td>0.242394</td>\n",
       "      <td>0.239683</td>\n",
       "      <td>0.262706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.260743</td>\n",
       "      <td>0.246983</td>\n",
       "      <td>0.239136</td>\n",
       "      <td>0.253138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.266471</td>\n",
       "      <td>0.234512</td>\n",
       "      <td>0.234461</td>\n",
       "      <td>0.264556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.254038</td>\n",
       "      <td>0.235937</td>\n",
       "      <td>0.236206</td>\n",
       "      <td>0.273818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.259680</td>\n",
       "      <td>0.240536</td>\n",
       "      <td>0.235796</td>\n",
       "      <td>0.263989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.247534</td>\n",
       "      <td>0.270575</td>\n",
       "      <td>0.232549</td>\n",
       "      <td>0.249341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.235854</td>\n",
       "      <td>0.272069</td>\n",
       "      <td>0.234150</td>\n",
       "      <td>0.257927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.240881</td>\n",
       "      <td>0.277129</td>\n",
       "      <td>0.233539</td>\n",
       "      <td>0.248451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>0.254417</td>\n",
       "      <td>0.250698</td>\n",
       "      <td>0.258082</td>\n",
       "      <td>0.236803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>0.242579</td>\n",
       "      <td>0.252256</td>\n",
       "      <td>0.260038</td>\n",
       "      <td>0.245126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.256902</td>\n",
       "      <td>0.259313</td>\n",
       "      <td>0.236079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>0.243874</td>\n",
       "      <td>0.243858</td>\n",
       "      <td>0.262809</td>\n",
       "      <td>0.249459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>0.232311</td>\n",
       "      <td>0.245146</td>\n",
       "      <td>0.264555</td>\n",
       "      <td>0.257988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>0.237419</td>\n",
       "      <td>0.249869</td>\n",
       "      <td>0.264038</td>\n",
       "      <td>0.248674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.242918</td>\n",
       "      <td>0.264565</td>\n",
       "      <td>0.247251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>0.233668</td>\n",
       "      <td>0.244234</td>\n",
       "      <td>0.266359</td>\n",
       "      <td>0.255739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>0.238784</td>\n",
       "      <td>0.248918</td>\n",
       "      <td>0.265815</td>\n",
       "      <td>0.246484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>0.244372</td>\n",
       "      <td>0.236681</td>\n",
       "      <td>0.260984</td>\n",
       "      <td>0.257963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>0.232735</td>\n",
       "      <td>0.237879</td>\n",
       "      <td>0.262661</td>\n",
       "      <td>0.266725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>0.237957</td>\n",
       "      <td>0.242570</td>\n",
       "      <td>0.262264</td>\n",
       "      <td>0.257208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>0.253741</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.266119</td>\n",
       "      <td>0.239114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>0.241907</td>\n",
       "      <td>0.242497</td>\n",
       "      <td>0.268105</td>\n",
       "      <td>0.247491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>0.247095</td>\n",
       "      <td>0.247038</td>\n",
       "      <td>0.267439</td>\n",
       "      <td>0.238428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0.255142</td>\n",
       "      <td>0.240054</td>\n",
       "      <td>0.267849</td>\n",
       "      <td>0.236954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>0.243276</td>\n",
       "      <td>0.241551</td>\n",
       "      <td>0.269884</td>\n",
       "      <td>0.245288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>0.248471</td>\n",
       "      <td>0.246053</td>\n",
       "      <td>0.269190</td>\n",
       "      <td>0.236286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>0.254329</td>\n",
       "      <td>0.233996</td>\n",
       "      <td>0.264343</td>\n",
       "      <td>0.247332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.242418</td>\n",
       "      <td>0.235376</td>\n",
       "      <td>0.266262</td>\n",
       "      <td>0.255944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>0.247722</td>\n",
       "      <td>0.239886</td>\n",
       "      <td>0.265714</td>\n",
       "      <td>0.246678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>0.250402</td>\n",
       "      <td>0.225771</td>\n",
       "      <td>0.278392</td>\n",
       "      <td>0.245435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>0.238635</td>\n",
       "      <td>0.227063</td>\n",
       "      <td>0.280364</td>\n",
       "      <td>0.253938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>0.243904</td>\n",
       "      <td>0.231460</td>\n",
       "      <td>0.279843</td>\n",
       "      <td>0.244792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>0.251769</td>\n",
       "      <td>0.224846</td>\n",
       "      <td>0.280183</td>\n",
       "      <td>0.243203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>0.239969</td>\n",
       "      <td>0.226163</td>\n",
       "      <td>0.282206</td>\n",
       "      <td>0.251662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>0.245246</td>\n",
       "      <td>0.230521</td>\n",
       "      <td>0.281656</td>\n",
       "      <td>0.242576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>0.250839</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>0.276376</td>\n",
       "      <td>0.253725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.220267</td>\n",
       "      <td>0.278275</td>\n",
       "      <td>0.262459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>0.244381</td>\n",
       "      <td>0.224628</td>\n",
       "      <td>0.277876</td>\n",
       "      <td>0.253115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unacc       acc     vgood      good\n",
       "0     0.258248  0.260228  0.222243  0.259281\n",
       "1     0.246134  0.261741  0.223838  0.268288\n",
       "2     0.251462  0.266696  0.223327  0.258515\n",
       "3     0.259809  0.259314  0.223804  0.257073\n",
       "4     0.247658  0.260859  0.225442  0.266041\n",
       "5     0.252995  0.265772  0.224906  0.256327\n",
       "6     0.258733  0.252528  0.220664  0.268075\n",
       "7     0.246540  0.253938  0.222197  0.277325\n",
       "8     0.251994  0.258866  0.221792  0.267347\n",
       "9     0.268838  0.257342  0.225160  0.248659\n",
       "10    0.256448  0.259061  0.226972  0.257519\n",
       "11    0.261854  0.263818  0.226327  0.248000\n",
       "12    0.270414  0.256390  0.226700  0.246496\n",
       "13    0.257988  0.258140  0.228556  0.255315\n",
       "14    0.263402  0.262856  0.227886  0.245856\n",
       "15    0.269418  0.249796  0.223622  0.257164\n",
       "16    0.256946  0.251411  0.225373  0.266270\n",
       "17    0.262480  0.256143  0.224834  0.256543\n",
       "18    0.266064  0.241747  0.236222  0.255967\n",
       "19    0.253707  0.243272  0.238033  0.264988\n",
       "20    0.259225  0.247901  0.237513  0.255361\n",
       "21    0.267609  0.240840  0.237824  0.253727\n",
       "22    0.255217  0.242394  0.239683  0.262706\n",
       "23    0.260743  0.246983  0.239136  0.253138\n",
       "24    0.266471  0.234512  0.234461  0.264556\n",
       "25    0.254038  0.235937  0.236206  0.273818\n",
       "26    0.259680  0.240536  0.235796  0.263989\n",
       "27    0.247534  0.270575  0.232549  0.249341\n",
       "28    0.235854  0.272069  0.234150  0.257927\n",
       "29    0.240881  0.277129  0.233539  0.248451\n",
       "...        ...       ...       ...       ...\n",
       "1698  0.254417  0.250698  0.258082  0.236803\n",
       "1699  0.242579  0.252256  0.260038  0.245126\n",
       "1700  0.247706  0.256902  0.259313  0.236079\n",
       "1701  0.243874  0.243858  0.262809  0.249459\n",
       "1702  0.232311  0.245146  0.264555  0.257988\n",
       "1703  0.237419  0.249869  0.264038  0.248674\n",
       "1704  0.245265  0.242918  0.264565  0.247251\n",
       "1705  0.233668  0.244234  0.266359  0.255739\n",
       "1706  0.238784  0.248918  0.265815  0.246484\n",
       "1707  0.244372  0.236681  0.260984  0.257963\n",
       "1708  0.232735  0.237879  0.262661  0.266725\n",
       "1709  0.237957  0.242570  0.262264  0.257208\n",
       "1710  0.253741  0.241026  0.266119  0.239114\n",
       "1711  0.241907  0.242497  0.268105  0.247491\n",
       "1712  0.247095  0.247038  0.267439  0.238428\n",
       "1713  0.255142  0.240054  0.267849  0.236954\n",
       "1714  0.243276  0.241551  0.269884  0.245288\n",
       "1715  0.248471  0.246053  0.269190  0.236286\n",
       "1716  0.254329  0.233996  0.264343  0.247332\n",
       "1717  0.242418  0.235376  0.266262  0.255944\n",
       "1718  0.247722  0.239886  0.265714  0.246678\n",
       "1719  0.250402  0.225771  0.278392  0.245435\n",
       "1720  0.238635  0.227063  0.280364  0.253938\n",
       "1721  0.243904  0.231460  0.279843  0.244792\n",
       "1722  0.251769  0.224846  0.280183  0.243203\n",
       "1723  0.239969  0.226163  0.282206  0.251662\n",
       "1724  0.245246  0.230521  0.281656  0.242576\n",
       "1725  0.250839  0.219060  0.276376  0.253725\n",
       "1726  0.239000  0.220267  0.278275  0.262459\n",
       "1727  0.244381  0.224628  0.277876  0.253115\n",
       "\n",
       "[1728 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def predict_unsupervised(class_distributions, unsupervised_model, instance_list, iterations=5):\n",
    "    \"\"\"\n",
    "    Predicts the class distribution for a set of\n",
    "    instances, based on a trained model. Delegates\n",
    "    the task of updating class distributions in a\n",
    "    single iteration to the train_unsupervised() function.\n",
    "    Returns the final class distributions for instances,\n",
    "    once all iterations have been completed.\n",
    "    \"\"\"\n",
    "        \n",
    "    # For each iteration, update the class distribution for each\n",
    "    # instance, in an attempt to make them more reliable than\n",
    "    # the initial random distribution.\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Calculate the frequency of the classes \n",
    "        # in the random class distribution\n",
    "        class_freqs = get_class_freqs2(class_distributions)\n",
    "         \n",
    "        \n",
    "        # Calculate the total number of instances\n",
    "        total_instances = int(sum(class_freqs.values()) + 0.5)\n",
    "    \n",
    "        \n",
    "        for i in range(len(instance_list)):\n",
    "\n",
    "            instance = instance_list.iloc[i,:]\n",
    "\n",
    "            for class_label in class_freqs:\n",
    "\n",
    "                # Firstly, get the prior probability of a class\n",
    "                prob = class_freqs[class_label] / total_instances\n",
    "\n",
    "                # Now, multiply the prior probability of the class\n",
    "                # by the posterior probability of each of the instance's \n",
    "                # attribute values, given the class.\n",
    "                for attr in unsupervised_model[class_label]:\n",
    "                    \n",
    "                    attr_value = instance[attr]\n",
    "                    \n",
    "                    # Simply not have missing values count\n",
    "                    # towards probability estimates.\n",
    "                    if attr_value == '?':\n",
    "                        continue\n",
    "                    \n",
    "                    prob *= (unsupervised_model[class_label][attr][attr_value] / class_freqs[class_label])\n",
    "\n",
    "\n",
    "                # Update the class distribution for the instance\n",
    "                class_distributions[class_label][i] = prob\n",
    "\n",
    "\n",
    "        # Now, normalise the class distributions so that the \n",
    "        # probabilities add to 1 for every single instance.\n",
    "        class_distributions = class_distributions.div(class_distributions.sum(axis=1), axis=0)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Update the unsupervised model based on the updated class distributions\n",
    "        class_distributions, unsupervised_model = train_unsupervised(instance_list, \n",
    "                                                                     class_distributions.columns, \n",
    "                                                                     class_distributions)\n",
    "        \n",
    "        \n",
    "    return class_distributions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_class_freqs2(class_distributions):\n",
    "    \"\"\"\n",
    "    Helper function that returns a dictionary\n",
    "    containing classes as keys, and the total\n",
    "    sum of the fractional values for the class\n",
    "    as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_freqs = defaultdict(float)\n",
    "    \n",
    "    for class_label in class_distributions.columns:\n",
    "        class_freqs[class_label] = class_distributions[class_label].sum()\n",
    "        \n",
    "    return class_freqs\n",
    "\n",
    "\n",
    "\n",
    "# Test the function\n",
    "class_distributions = predict_unsupervised(cd, um, instance_list)\n",
    "class_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Unsupervised Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "================\n",
      "unacc               :   defaultdict(<class 'int'>, {'unacc': 331, 'acc': 79, 'good': 7, 'vgood': 14})\n",
      "acc                 :   defaultdict(<class 'int'>, {'unacc': 401, 'acc': 118, 'vgood': 20, 'good': 24})\n",
      "vgood               :   defaultdict(<class 'int'>, {'unacc': 236, 'acc': 128, 'vgood': 24, 'good': 28})\n",
      "good                :   defaultdict(<class 'int'>, {'unacc': 242, 'acc': 59, 'vgood': 7, 'good': 10})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7002314814814815"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate_unsupervised(class_distributions, class_list):\n",
    "    \"\"\"\n",
    "    Evaluates a set of predictions, in an\n",
    "    unsupervised manner. Takes as input a set of \n",
    "    final class distributions for each instance, and \n",
    "    the actual class for each respective instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation checking\n",
    "    assert(len(class_distributions) == len(class_list))\n",
    "    \n",
    "    \n",
    "    # The list of class predictions for the instances\n",
    "    prediction_list = []\n",
    "    \n",
    "    \n",
    "    # Assign the class with the highest probability in\n",
    "    # the final distribution as the predicted class, and\n",
    "    # add it to the list of predictions.\n",
    "    for distribution in class_distributions.values:\n",
    "        \n",
    "        predicted_class = class_distributions.columns[np.argmax(distribution)]\n",
    "        \n",
    "        prediction_list.append(predicted_class)\n",
    "            \n",
    "    \n",
    "    # Initialise and fill in the confusion matrix\n",
    "    confusion_matrix = {predicted_class : defaultdict(int) for predicted_class in set(prediction_list)}\n",
    "    \n",
    "    for i in range(len(prediction_list)):\n",
    "        predicted_class = prediction_list[i]\n",
    "        actual_class = class_list[i]\n",
    "        confusion_matrix[predicted_class][actual_class] += 1\n",
    "    \n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix\\n================\") #Predicted |   Actual\\n\" + 100*\"-\")\n",
    "    for predicted_class in confusion_matrix:\n",
    "        print(\"{:20s}:\".format(predicted_class) + \"   {0}\".format(confusion_matrix[predicted_class]))\n",
    "    \n",
    "    \n",
    "    # Calculate the total number of 'correct' predictions. We\n",
    "    # take the max value from each column in the confusion matrix,\n",
    "    # and add it to the total number of correct predictions.\n",
    "    total_correct = 0\n",
    "    for predicted_class in confusion_matrix:\n",
    "        total_correct += max(confusion_matrix[predicted_class].values())\n",
    "    \n",
    "    \n",
    "    # Finally, calculate accuracy of the classifier\n",
    "    accuracy = total_correct / len(prediction_list)\n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Test the function\n",
    "evaluate_unsupervised(class_distributions, class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Functions\n",
    "These functions are drivers for both supervised and unsupervised Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "================\n",
      "hypothyroid         :   defaultdict(<class 'int'>, {'hypothyroid': 4, 'negative': 453})\n",
      "negative            :   defaultdict(<class 'int'>, {'hypothyroid': 147, 'negative': 2559})\n",
      "0.9522605121719886\n"
     ]
    }
   ],
   "source": [
    "# Driver for supervised Naive Bayes classification\n",
    "\n",
    "def supervised_naive_bayes(filename):\n",
    "    \"\"\"\n",
    "    Creates a supervised Naive Bayes model\n",
    "    given a set of input data, classifies\n",
    "    the instances based on the probabilistic \n",
    "    model, then prints the accuracy of the \n",
    "    classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    instance_list, class_list = preprocess(filename)\n",
    "    supervised_model = train_supervised(instance_list, class_list)\n",
    "    prediction_list = predict_supervised(supervised_model, instance_list)\n",
    "    accuracy = evaluate_supervised(prediction_list, class_list)\n",
    "    print(accuracy)\n",
    "    \n",
    "    \n",
    "# Driver for unsupervised Naive Bayes classification\n",
    "    \n",
    "def unsupervised_naive_bayes(filename):\n",
    "    \"\"\"\n",
    "    Creates a weak unsupervised Naive Bayes model\n",
    "    given a set of training instances, classifies\n",
    "    the instances by iteratively updating an initial\n",
    "    set of random class distributions, then\n",
    "    prints the accuracy of the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    instance_list, class_list = preprocess(filename)\n",
    "    class_labels = class_list.unique()\n",
    "    class_distributions, unsupervised_model = train_unsupervised(instance_list, class_labels)\n",
    "    final_class_distributions = predict_unsupervised(class_distributions, unsupervised_model, instance_list)\n",
    "    accuracy = evaluate_unsupervised(final_class_distributions, class_list)\n",
    "    print(accuracy)\n",
    "    \n",
    "    \n",
    "# Filename constants used for easy file access\n",
    "CSV1 = 'breast-cancer-dos.csv'\n",
    "CSV2 = 'car-dos.csv'\n",
    "CSV3 = 'hypothyroid-dos.csv'\n",
    "CSV4 = 'mushroom-dos.csv'\n",
    "\n",
    "\n",
    "# Testing driver functions\n",
    "filename = CSV3\n",
    "\n",
    "unsupervised_naive_bayes(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. Since we’re starting off with random guesses, it might be surprising that the unsupervised NB works at all. Explain what characteristics of the data cause it to work pretty well (say, within 10% Accuracy of the supervised NB) most of the time; also, explain why it utterly fails sometimes.\n",
    "2. When evaluating supervised NB across the four different datasets, you will observe some variation in effectiveness (e.g. Accuracy). Explain what causes this variation. Describe and explain any particularly suprising results.\n",
    "3. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out (hint: check out numpy.shuffle()) or cross–validation evaluation strategy. How does your estimate of Accuracy change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "4. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Do you notice any variation in the predictions made by either the supervised or unsupervised NB classifiers? Explain why, or why not.\n",
    "5. The lecture suggests that deterministically labelling the instances in the initialisation phase of the unsupervised NB classifier “doesn’t work very well”. Confirm this for yourself, and then demonstrate why.\n",
    "6. Rather than evaluating the unsupervised NB classifier by assigning a class deterministically, instead calculate how far away the probabilistic estimate of the true class is from 1 (where we would be certain of the correct class), and take the average over the instances. Does this performance estimate change, as we alter the number of iterations in the method? Explain why.\n",
    "7. Explore what causes the unsupervised NB classifier to converge: what proportion of instances change their prediction from the random assignment, to the first iteration? From the first to the second? What is the latest iteration where you observe a prediction change? Make some conjecture(s) as to what is occurring here.\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question. Groups of 2 students should respond to question (1), and three other questions. Your responses should be about 100-200 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "\n",
    "### Question 1\n",
    "Since we’re starting off with random guesses, it might be surprising that the unsupervised NB works at all. Explain what characteristics of the data cause it to work pretty well (say, within 10% Accuracy of the supervised NB) most of the time; also, explain why it utterly fails sometimes.\n",
    "\n",
    "\n",
    "Answer: _The unsupervised classifier works well on datasets that have both a small number of distinct classes, and a small number of distict values for their attributes. Furthermore, the classifier achieves lower variance in accuracy for datasets containing instances that are partitioned non-uniformly among classes._\n",
    "\n",
    "_For example, the classifier shows high variance in accuracy for the mushroom data with a mean of 0.813, a low of 0.579 and a high of 0.895. This variance could be the result of many attributes having a high number of unique values, with a max of 12 unique values for attribute 8, and a mean of 5.32 unique values per attribute. Compounded with the fact that the training data is partitioned pretty evenly among two classes (4208 instances of 'e', and 3916 of 'p'), this could have made it difficult for the classifier to converge to a particular set of predictions, hence the randomness of results._\n",
    "\n",
    "_Juxtaposed, consistent results are shown for the hypothyroid and breast cancer datasets, with mean accuracies of 0.952 and 0.703 respectively. 17 of 18 attributes in the hypothyroid data have only 2 possible unique values. A possible further reason for low variance could be that the distribution of instances among classes is highly non-uniform for both datasets. This could have a propensity to inflate accuracy, since accuracy is derived by taking the maximum value from each column in the confusion matrix._\n",
    "\n",
    "\n",
    "**Note to assessor: corresponding accuracies for supervised NB are detailed in question two.**\n",
    "\n",
    "### Question 2\n",
    "When evaluating supervised NB across the four different datasets, you will observe some variation in effectiveness (e.g. Accuracy). Explain what causes this variation. Describe and explain any particularly suprising results.\n",
    "\n",
    "\n",
    "Answer: _The accuracies of the supervised classifier across the datasets can be seen below:_\n",
    "\n",
    "| Dataset        | Accuracy      |\n",
    "| -------------  |:-------------:|\n",
    "| _breast-cancer_| 0.755         | \n",
    "| _car_          | 0.874         |\n",
    "| _hypothyroid_  | 0.952         |\n",
    "| _mushroom_     | 0.997         |\n",
    "\n",
    "\n",
    "_Since accuracy measures the ratio of correctly classified instances across all classes, this means that if one class occurs more than others, then the resulting accuracy is clearly dominated by the accuracy of the dominating class. This is the case for the hypothyroid dataset, since the classifier predicts 'negative' for all instances, when the training set is dominated by 'negative' instances._\n",
    "\n",
    "_A surprising result was the accuracy of the classifier for the mushroom data. Such a high accuracy was not expected since 30.5% of instances contained at least one missing value. The reason a high accuracy was received is due to there being a large number of training instances containing a large distribution of possible attribute value-class pairs, and because Naive Bayes gracefully degrades when one or more predictor variables are missing or not observed. The latter is due to the implementation of epsilon smoothing and simply not having missing values contribute to frequency counts for their respective features._\n",
    "\n",
    "_Compared to the other results, the accuracy of 0.755 for the breast cancer data was relatively low. Unlike the other datasets, the training set was relatively small, perhaps leading to a probabilistic model that did not cover a wide spectrum of attribute value-class pairs, possibly ultimately leading to a moderately low prediction accuracy._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOL\n",
    "\n",
    "_A plausible but highly unlikely reason for this could be that the data could contain highly correlated attributes such as a, which may degrade the performance of supervised Naive Bayes classification, because the conditional independence assumption does not hold anymore._\n",
    "\n",
    "_Even though the probability estimates of Naive Bayes are of low quality, its classification decisions are surprisingly good. NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. Byt tge ckassufucatuin decusuib us vased ib wgucg class gets the highest score. It does not matter how accurate the estimates are. Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
